{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed071b-536e-44be-a1f0-94d8880b7267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c676f2-d517-4cb3-8ef0-9b44a766dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mpnn_repository = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2e4f2-bbc5-4ece-96ca-7acf2a77dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to diversify Specified Contigs\n",
    "def expand_list(lists_of_lists, range_expand, sample_size, use_expanded=True, prefix_limit=120, sample=False):\n",
    "    def expand_sequence(sequence, range_expand):\n",
    "        expanded_sequence = set(sequence)  # Use a set to avoid duplicates\n",
    "        for item in sequence:\n",
    "            match = re.match(r\"([A-Z])(\\d+)\", item)  # Parse \n",
    "            if match:\n",
    "                prefix, num = match.groups()\n",
    "                num = int(num)\n",
    "                # Expand within range\n",
    "                for i in range(1, range_expand + 1):\n",
    "                    # Expand upwards\n",
    "                    if num + i <= prefix_limit:\n",
    "                        expanded_sequence.add(f\"{prefix}{num + i}\")\n",
    "                    # Expand downwards\n",
    "                    if num - i > 0:  # Assuming numbers > 0 are valid\n",
    "                        expanded_sequence.add(f\"{prefix}{num - i}\")\n",
    "        return list(expanded_sequence)\n",
    "\n",
    "    # Expand the lists based on the range_expand and limit conditions\n",
    "    expanded_lists = [expand_sequence(sub_list, range_expand) for sub_list in lists_of_lists]\n",
    "\n",
    "    # If use_expanded is True, sample from the expanded lists, otherwise sample from original\n",
    "    if sample:\n",
    "        sampled_lists = []\n",
    "        for i, sub_list in enumerate(expanded_lists if use_expanded else lists_of_lists):\n",
    "            if sample_size > 0:\n",
    "                sampled_lists.append(random.sample(sub_list, min(sample_size, len(sub_list))))\n",
    "            else:\n",
    "                sampled_lists.append(sub_list)\n",
    "    \n",
    "        # Join all lists into one list and return the joined string\n",
    "        joined_list = ' '.join([item for sublist in sampled_lists for item in sublist])\n",
    "    else:\n",
    "        joined_list = ' '.join([item for sublist in expanded_lists for item in sublist])\n",
    "    \n",
    "    return joined_list\n",
    "\n",
    "# Test usage:\n",
    "lists_of_lists = [[\"B99\", \"B100\", \"B101\"], [\"D105\", \"D106\", \"D107\"], [\"D105\", \"D106\", \"D107\"]]\n",
    "result = expand_list(lists_of_lists, range_expand=5, sample_size=3, use_expanded=True, sample=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d01146-1d78-4075-acb4-c9c96dde00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_list(lists_of_lists, range_expand=7, sample_size=3, use_expanded=True, sample=False)  # test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4202db-d192-4929-ba72-f6539f8df029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nanobody_sample(t, sc_context, pdb_path,chain_parse, fixed = None, model=None, extra=None):\n",
    "    base_name = os.path.splitext(os.path.basename(pdb_path))[0]\n",
    "\n",
    "    # Output and save path modifications included design params and unique identifiers for running in a loop\n",
    "    if fixed:\n",
    "        output = f\"fixed_cdr_t_{t}\"\n",
    "    else:\n",
    "        output = f\"t_{t}\"\n",
    "\n",
    "\n",
    "\n",
    "    command = [\n",
    "        'python', f'/{path_to_mpnn_repository}/LigandMPNN/run.py', \n",
    "        '--pdb_path', f'{pdb_path}',        \n",
    "        '--batch_size', str(1),#str(1024), # increase number of redesigns       \n",
    "        '--number_of_batches', str(1), #str(200),\n",
    "        '--temperature', str(t),\n",
    "        '--parse_these_chains_only', chain_parse]\n",
    "    if fixed:\n",
    "        command = command + ['--redesigned_residues', fixed]  # redesign the selected residues\n",
    "        print(fixed)\n",
    "\n",
    "    if sc_context:\n",
    "        command = command + ['--ligand_mpnn_use_side_chain_context' ,str(1) , '--model_type', \"ligand_mpnn\"]\n",
    "        output = \"sc_ligand_mpnn_\" + output\n",
    "    if model:\n",
    "        command = command + ['--model_type', model]\n",
    "        output = f\"{model}_\" + output\n",
    "    if extra:\n",
    "        output = extra + output\n",
    "    save_path = os.path.join(base_name+\"_test\", output+\"_round_4\")\n",
    "    command = command + ['--out_folder', f\"mpnn_generated/{save_path}\"]\n",
    "    print(command)\n",
    "    if os.path.exists(save_path):  #if interupted dont regenerate\n",
    "        return\n",
    "    else:\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(\"Output:\", result.stdout)\n",
    "        print(\"Error:\", result.stderr)\n",
    "# Variable regions\n",
    "fixed_res_4krl = [[\"B27\",\"B28\",\"B29\",\"B30\",\"B31\",\"B32\",\"B34\",\"B35\"],[\"B50\",\"B51\",\"B52\",\"B53\",\"B54\",\"B55\",\"B56\",\"B57\"],[\"B99\",\"B100\",\"B101\",\"B102\",\"B103\",\"B104\",\"B105\",\"B106\",\"B107\",\"B108\",\"B109\",\"B110\",\"B111\",\"B112\",\"B113\"]]\n",
    "\n",
    "# Potentially Fixed regions\n",
    "fw_4krl = [[f\"B{i+1}\" for i in range(26)], [f\"B{i+1}\" for i in range(35, 49)],\n",
    "           [f\"B{i+1}\" for i in range(57, 98)], [f\"B{i+1}\" for i in range(113, 122)]]\n",
    "\n",
    "\n",
    "\n",
    "def generate_combinations(lists_3, lists_4):\n",
    "    # Ensure that lists_3 has exactly 3 sublists and lists_4 has exactly 4 sublists\n",
    "    # CDRs for lists_3 and Frameworks for lists_4\n",
    "    assert len(lists_3) == 3, \"should have 3 sublists\"\n",
    "    assert len(lists_4) == 4, \"should have 4 sublists\"\n",
    "\n",
    "    # Fixed sublists from lists_3 (positions 0 and 2)\n",
    "    fixed_sublists = [lists_3[0], lists_3[2]]\n",
    "    \n",
    "    # Collect all possible combinations of the middle sublist (position 1 of lists_3) and all sublists from lists_4\n",
    "    all_optional_sublists = lists_3[1:2] + lists_4  # Middle sublist (index 1) + all sublists from lists_4\n",
    "    \n",
    "    # Get all subsets (including the empty set) of the optional sublists\n",
    "    all_combinations_of_optional = []\n",
    "    for r in range(len(all_optional_sublists) + 1):\n",
    "        all_combinations_of_optional.extend(itertools.combinations(all_optional_sublists, r))\n",
    "    \n",
    "    # Initialize a list to store the final combinations\n",
    "    all_combinations = []\n",
    "    \n",
    "    # For each combination of optional sublists, combine it with the fixed sublists\n",
    "    for optional_subset in all_combinations_of_optional:\n",
    "        combination = fixed_sublists + list(optional_subset)\n",
    "        \n",
    "        # Create a compact identifier based on the included sublists\n",
    "        included_lists = [f\"L3_0\", f\"L3_2\"]  # Always include these\n",
    "        for i, sublist in enumerate(optional_subset):\n",
    "            if sublist in lists_3[1:2]:\n",
    "                included_lists.append(f\"L3_1\")\n",
    "            else:\n",
    "                index_in_l4 = lists_4.index(sublist)\n",
    "                included_lists.append(f\"L4_{index_in_l4}\")\n",
    "        \n",
    "        identifier = \"_\".join(included_lists)\n",
    "        \n",
    "        # Append the combination and its identifier to the result list\n",
    "        all_combinations.append((identifier, combination))\n",
    "    \n",
    "    return all_combinations\n",
    "\n",
    "\n",
    "combinations = generate_combinations(fixed_res_4krl, fw_4krl)  # generate combinations\n",
    "pdb_path_4krl = \"4krl.pdb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989c665-ed0d-4690-b2ac-93b25987e477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through all design parameters \n",
    "for pdb_path, fixed_res in zip([pdb_path_4krl], [fixed_res_4krl]):\n",
    "    for temp in [2,3]:\n",
    "        for unique, combo in combinations + [('', False)]:\n",
    "            for fixed in [True, False]:\n",
    "                for sc in [False]:\n",
    "                    for model in [\"soluble_mpnn\", \"ligand_mpnn\"]:\n",
    "                        if combo == False:\n",
    "                            if fixed:\n",
    "                                for expand in [0, 3]:\n",
    "                                    if expand == 0:\n",
    "                                        expand_bool=False\n",
    "                                    else:\n",
    "                                        expand_bool=True\n",
    "                                    for sample in [0, 5]:\n",
    "                                        if sample == 0:\n",
    "                                            fixed_res2 = expand_list(fixed_res, expand, sample, expand_bool, prefix_limit=120, sample=False)\n",
    "                                        else:\n",
    "                                            fixed_res2 = expand_list(fixed_res, expand, sample, expand_bool, prefix_limit=120, sample=True)\n",
    "                                nanobody_sample(temp, sc, pdb_path,\"B\", fixed = fixed_res2,model=model, extra=f\"expand_{expand}_sample_{sample}_\")\n",
    "                            else:  \n",
    "                                nanobody_sample(temp, sc, pdb_path, \"B\", model=model)\n",
    "                        else:\n",
    "                            if fixed:\n",
    "                                \n",
    "                                for expand in [0, 3]:\n",
    "                                    if expand == 0:\n",
    "                                        expand_bool=False\n",
    "                                    else:\n",
    "                                        expand_bool=True\n",
    "                                    for sample in [0, 5]:\n",
    "                                        if sample == 0:\n",
    "                                            fixed_res2 = expand_list(combo, expand, sample, expand_bool, prefix_limit=120, sample=False)\n",
    "                                        else:\n",
    "                                            fixed_res2 = expand_list(combo, expand, sample, expand_bool, prefix_limit=120, sample=True)\n",
    "                                nanobody_sample(temp, sc, pdb_path,\"B\", fixed = fixed_res2,model=model, extra=f\"expand_{expand}_sample_{sample}_combo_{unique}_\")\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce40472-0241-4b61-aad2-591f7c0289b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse a the results into a df\n",
    "def parse_fasta(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Skip the first two lines\n",
    "        lines = lines[2:]\n",
    "        \n",
    "        data = []\n",
    "        for i in range(0, len(lines), 2):  # Process every pair of lines\n",
    "            header = lines[i].strip()\n",
    "            sequence = lines[i + 1].strip()\n",
    "            \n",
    "            # Extract information from header\n",
    "            header_info = {}\n",
    "            header_parts = header.split(',')\n",
    "            header_info['name'] = header_parts[0].replace('>', '').strip()\n",
    "            header_info['id'] = header_parts[1].split('=')[1].strip()\n",
    "            header_info['T'] = float(header_parts[2].split('=')[1].strip())\n",
    "            header_info['seed'] = header_parts[3].split('=')[1].strip()\n",
    "            header_info['overall_confidence'] = float(header_parts[4].split('=')[1].strip())\n",
    "            header_info['ligand_confidence'] = float(header_parts[5].split('=')[1].strip())\n",
    "            header_info['seq_rec'] = float(header_parts[6].split('=')[1].strip())\n",
    "            header_info['sequence'] = sequence\n",
    "            \n",
    "            data.append(header_info)\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd69e1-561b-47ba-a59a-3a2a1e92a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20b084-d4be-4f6a-8fcd-ffea29198ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"mpnn_generated/bioml\"):\n",
    "    for file in files:\n",
    "        if file.endswith('.fa'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            if'.ipynb' not in file_path:\n",
    "                print(f\"Processing {file_path}\")\n",
    "                fasta_data = parse_fasta(file_path)\n",
    "                \n",
    "                # Convert to DataFrame and append to list\n",
    "                df = pd.DataFrame.from_records(fasta_data)\n",
    "                df[\"file_path\"] = file_path\n",
    "                dataframes.append(df)\n",
    "\n",
    "\n",
    "#all_data = pd.concat(dataframes, ignore_index=True)\n",
    "# Save to CSV\n",
    "#all_data.to_csv(f'{base_name}_generated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1f71d-ea65-4872-bd9f-0f5529aca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for df in dataframes:\n",
    "    df = df[df[\"seq_rec\"]>=0.60]  # filter by a sequence recovery threshold\n",
    "    df = df.sort_values(by=[\"overall_confidence\", \"seq_rec\"], ascending=False)  # sort by metrics\n",
    "    df = df.drop_duplicates(subset=\"sequence\")  # drop duplicates\n",
    "    if not df.empty:\n",
    "        results.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e342e-ea4e-4ac9-b247-a5f6de87fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(results)\n",
    "results = results.sort_values(by=[\"overall_confidence\", \"seq_rec\"], ascending=False)\n",
    "results = results.drop_duplicates(subset=\"sequence\")  # drop duplicates\n",
    "results.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341e1ae-fe80-48ff-a52f-2d3c51cf88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp.fasta\", \"w\") as f:  # write temporary fasta\n",
    "\n",
    "    for i, row in results_df2.iterrows():\n",
    "        f.write(f\">{i}\\n\")\n",
    "        f.write(f\"{row.sequence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556d26b-9893-4599-9e64-a69c07a37abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sadie.renumbering import Renumbering\n",
    "# We wrap these in a function so we can use multiprocessing\n",
    "def sadie_run() -> pd.DataFrame:\n",
    "    # setup API  object\n",
    "    renumbering_api = Renumbering(scheme=\"chothia\", region_assign=\"imgt\", run_multiproc=True)\n",
    "\n",
    "    # run the renumbering on a file\n",
    "    numbering_table = renumbering_api.run_file(fasta)\n",
    "\n",
    "    return numbering_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845dbe44-60d2-4903-a971-7edff3a6d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "sadie_results = sadie_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad0c1e-0e32-47d9-974c-c10c1e43fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.merge(sadie_results, on =\"sequence\", how=\"left\")\n",
    "results = results.sort_values(by=[\"overall_confidence\", \"seq_rec\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f73cd-1446-4c3e-9f76-66f737699556",
   "metadata": {},
   "outputs": [],
   "source": [
    "results =results.drop_duplicates(subset=\"cdr3_aa_no_gaps\") # Filter on CDR1 and CDR3 as they are the parent contacts to the antigen\n",
    "results =results.drop_duplicates(subset=\"cdr1_aa_no_gaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ebb93-fd12-411e-866b-3af6128cc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the columns and names\n",
    "results[\"method\"] = [\"soluble_mpnn\" if \"soluble\" in i else i for i in results[\"file_path\"]]\n",
    "results[\"method\"] = [\"ligand_mpnn\" if \"ligand\" in i else i for i in results[\"method\"]]\n",
    "results[\"method\"] = [\"protein_mpnn\" if \"protein\" in i else i for i in results[\"method\"]]\n",
    "results=results.rename(columns={\"name\": \"parent\"})\n",
    "results[\"parameters\"] = [i.split(\"/\")[-3].replace(\".fa\", \"\") for i in results[\"file_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10809abe-11ae-44ad-a0c2-91bcdb52e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"method\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c2b77-65b6-42bd-b2c1-51ece530a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"mpnn_strict_filtered.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ligandmpnn_env]",
   "language": "python",
   "name": "conda-env-ligandmpnn_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
